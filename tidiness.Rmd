---
title: "Tidiness: A measure based on information theory to help with
  selecting an appropriate number of dimensions to extract in MDA"
output:
  html_notebook:
    number_sections: yes
    toc: yes
---

**NB:** This is complementary material to Section 4 of Cvrček et. al
(2018), which presents a more accessible conceptual background to the
rather technical discussion below.

Introduction
============

In *multi-dimensional analysis* (MDA; Biber 1988), linguistic features
are grouped into dimensions of variation based on the texts in which
they co-occur, via a statistical procedure called [*factor
analysis*](https://en.wikipedia.org/wiki/Factor_analysis) (FA). One
input parameter to FA is the **number of factors to extract**, which are
then interpreted as dimensions. Consequently, this parameter has a
considerable influence on the picture of language variation that emerges
from the analysis.

Many heuristic methods designed to help with making that choice exist,
all of them with advantages and disadvantages:

> Using either the chi square test or the change in square test is, of
> course, sensitive to the number of subjects and leads to the
> nonsensical condition that if one wants to find many factors, one
> simply runs more subjects. Parallel analysis is partially sensitive to
> sample size in that for large samples (N > 1,000) all of the eigen
> values of random factors will be very close to 1. The scree test is
> quite appealing but can lead to differences of interpretation as to
> when the scree “breaks”. Extracting interpretable factors means that
> the number of factors reflects the investigators creativity more than
> the data. vss, while very simple to understand, will not work very
> well if the data are very factorially complex. (Simulations suggests
> it will work fine if the complexities of some of the items are no more
> than 2). The eigen value of 1 rule, although the default for many
> programs, seems to be a rough way of dividing the number of variables
> by 3 and is probably the worst of all criteria.
>
> ([Revelle 2017: 38](http://personality-project.org/r/psych/HowTo/factor.pdf))

In practical, real-world settings, some of these methods can often be
ruled out right off the bat. Take the example of our MDA of Czech,
described e.g. in XX (ZZZZ): since the number of observations is 3000+,
we cannot use parallel analysis; mean item complexity of 2.3 rules out
vss. One popular choice in the context of MDA is to manually compare
several plausible candidate models (with different numbers of factors)
and select the one which yields the best interpretation (with the
obvious disadvantage of the subjectivity of such a decision, cf.
*extracting interpretable factors* above).

Considering that other MDA projects may encounter similar obstacles, we
suggest a procedure designed to aid this manual comparison. It consists
in a quantitative reformulation of some of the comparison criteria, thus
minimizing the subjectivity of the decision, especially in situations
where no clear winner is readily apparent.

The procedure can be summarized thus:

1. Perform [hierarchical cluster analysis
   (HCA)](http://www.r-tutor.com/gpu-computing/clustering/hierarchical-cluster-analysis)
   on the features based on their *absolute* loadings on all factors of
   all candidate models. This gives us an idea of which regularities
   tend to hold across all proposed solutions: e.g. features $A$ and $B$
   may tend to always load heavily on the same dimension, so they emerge
   from the HCA as close to one another, whereas $C$ always loads most
   prominently elsewhere.
2. For each $f$-factor model, generate a corresponding $f$-cluster
   partition of the HCA output (by pruning it).
3. Take every possible pair of $i$-factor model and $j$-cluster
   partition and compare how they structure the features into factors /
   clusters. Is the mapping straightforward (tidy) or messy (tangled)?
   Quantify that using a measure, call it *tidiness* (see more below).
4. Use the performance of the individual candidate models across the
   comparisons in 3 to inform your choice of which one to stick with: a
   model which tends to come out of these comparisons as tidier than its
   peers is one that better represents the robust feature-grouping
   trends inferred via HCA, which is an argument in its favor.

This document focuses on step 3: how to operationalize *tidiness*. We
start by giving an abstract definition, but if it feels dense, you may
want to skip ahead to the [concrete example with code](#implementation)

Defining *tidiness*
===================

<center>
![Fig. Visual motivation for the definition of *tidiness*.](tidiness.png){ width=60% }
</center>

*Tidiness* is based on information-theoretic measures: the [*mutual
information*](https://en.wikipedia.org/wiki/Mutual_information) of a
random variable ($I(X; Y)$) and the [*joint
entropy*](https://en.wikipedia.org/wiki/Joint_entropy) of two random
variables ($H(X, Y)$). It quantifies how much information about one
grouping (e.g. factors) can be inferred from knowing the other grouping
(e.g. clusters), relative to how much information the entire system
consisting of both groupings contains:

$$
tidiness(X, Y) = \frac{I(X; Y)}{H(X,Y)}
$$

where, as per the definitions of mutual information and joint entropy:

$$
\begin{align*}
I(X; Y) &= \sum_{y \in Y}\sum_{x \in X} p(x, y) \log_2 \left( \frac{p(x,y)}{p(x) p(y)} \right) \\
H(X, Y) &= - \sum_{y \in Y}\sum_{x \in X} p(x, y) \log_2 \left[ p(x,y) \right] \\
\end{align*}
$$

Furthermore, $p(x,y)$ is the [joint
probability](https://en.wikipedia.org/wiki/Joint_probability_distribution)
of $X$ and $Y$; $p(x)$ and $p(y)$ are the [marginal
probability](https://en.wikipedia.org/wiki/Marginal_distribution) of $X$
and $Y$ respectively. Finally, $X$ is an $i$-cluster partition and $Y$
is a $j$-factor model.

Implementation
==============

Example data
------------

The objective is to compare two groupings (clusters and factors) of the
same objects (features), assessing how tidy the mapping from one to the
other is.

In one of the groupings -- clusters -- group membership is binary and
exclusive: a feature either belongs to a cluster or not. Say we have 4
features and 2 clusters. We can generate a table that tells us, for
every possible combination of feature and cluster, whether that feature
belongs to the cluster or not:

```{r message=FALSE, warning=FALSE}
library(tidyverse)

clusters <- expand.grid(FEATURE=LETTERS[1:4], CLUSTER=c("c1", "c2")) %>% as_tibble()
clusters$BELONGS_TO_CLUSTER <- c(1, 1, 0, 0, 0, 0, 1, 1)
clusters
```

$\Rightarrow$ Features `A` and `B` belong to cluster `c1`, `C` and `D`
to `c2`.

With factors, group membership is gradual. Each feature "belongs" to
**all** of the factors to different extents, as quantified by its
different loadings on those factors. Again, here's a possible table for
4 features and 2 factors (it could have been a different number, in
general, we want to be able to compare any $i$-cluster partition with
any $j$-factor model; only the number of features remains the same):

```{r}
factors <- expand.grid(FEATURE=LETTERS[1:4], FACTOR=c("f1", "f2")) %>% as_tibble()
factors$LOADING_ON_FACTOR <- c(.1, .05, .6, -.7, -.5, .55, -.02, .2)
factors
```

$\Rightarrow$ Features `A` and `B` have a tendency to load more heavily
on (= belong "more" to) factor `f2`, `C` and `D` to `f1`.

We can now merge the information about both groupings into a single
table:

```{r}
data <- inner_join(clusters, factors, by="FEATURE")
data
```

The observed occurrences of features within clusters are represented by
the `BELONGS_TO_CLUSTER` column, the correlations between features and
factors are indicated in the `LOADING_ON_FACTOR` column. Each row
represents an event where a given feature was found in a given cluster
and a given factor at the same time.

To compute the weight of that event (its unnormalized probability),
multiply `BELONGS_TO_CLUSTER` by the **absolute value** of
`LOADING_ON_FACTOR`. The absolute value is used because we only care
about the **strength** of the correlation between a factor and a
feature, not its **direction**.

```{r}
weighted <- mutate(data, WEIGHT=BELONGS_TO_CLUSTER * abs(LOADING_ON_FACTOR)) %>%
  select(-BELONGS_TO_CLUSTER, -LOADING_ON_FACTOR)
weighted
```

As expected, only rows where the features are with the cluster where
they actually occurred contribute a non-zero `WEIGHT` value. We can
therefore clean up the table to make it smaller:

```{r}
non_zero <- filter(weighted, WEIGHT > 0)
non_zero
```

If later on, you find yourself wondering, "But what happened e.g. to the
events where feature `A` is seen in cluster `c2`, shouldn't these
contribute to the calculation as well?", well they do in theory, but
their contribution is defined as 0, because `A` belongs to `c1`, not
`c2`, so the weight of these events is 0 and we can afford to discard
them early.

From weights to probabilities
-----------------------------

We're now interested in the mapping between clusters and factors: does
the group of features in `c1` map more or less straightforwardly onto
the features that load heavily on `f1`, or `f2` (= tidy), or conversely,
is it maybe split between the two (= tangled)? In the tangled case, it's
harder to predict, given that a feature belongs to cluster `c1`, whether
it will load more heavily on factor `f1` or `f2`.

In order to assess this, we can aggregate the weights for every
combination of `CLUSTER` and `FACTOR`:

```{r}
aggregated <- select(non_zero, -FEATURE) %>%
  group_by(CLUSTER, FACTOR) %>%
  summarize(WEIGHT=sum(WEIGHT)) %>%
  ungroup()
aggregated
```

$\Rightarrow$ The correspondence between `c1` and `f1` is weak, so it
doesn't compete very much with the strong correspondence between `c1`
and `f2`.

We can re-arrange this in more compact form as a `CLUSTER` × `FACTOR`
table...

```{r}
rearranged <- spread(aggregated, FACTOR, WEIGHT) %>%
  as.data.frame() %>%
  remove_rownames() %>%
  column_to_rownames("CLUSTER") %>%
  as.matrix()
rearranged
```

... and normalize the weights to yield probabilities, pre-computing the
joint and marginal probability distributions:

```{r}
joint <- prop.table(rearranged)
joint
```

```{r}
marginal_rows <- margin.table(joint, 1)
marginal_rows
```

```{r}
marginal_columns <- margin.table(joint, 2)
marginal_columns
```

Computing mutual information, joint entropy and *tidiness*
----------------------------------------------------------

We now have all we need to fill out the equations given
[above](#defining-tidiness). First, mutual information ($I$):

```{r}
# the comments refer back to the equation above
I <- 0
# outer Σ: for cluster x in i-cluster partition X
for (rn in rownames(joint)) {
  # inner Σ: for factor y in j-factor model Y
  for (cn in colnames(joint)) {
    # joint probability of cluster x and factor y
    j <- joint[rn, cn]
    # marginal probability of cluster x
    mr <- marginal_rows[rn]
    # marginal probability of factor y
    mc <- marginal_columns[cn]
    I <- I + j * log2(j / (mr * mc))
  }
}
I <- unname(I)
I
```

Now, joint entropy ($H$):

```{r}
H <- 0
for (j in joint) {
  H <- H - j * log2(j)
}
H
```

And finally, *tidiness*:

```{r}
TIDINESS <- I / H
TIDINESS
```

Of course, one tidiness number on its own isn't all that useful, we want
to use it to *compare* models. We'll try some examples
[below](#testing-if-it-works), after we've defined appropriate functions
to encapsulate the calculations sketched above.

Wrapping all of this in functions
---------------------------------

Our `tidiness()` function will accept a data frame in the format of the
`data` data frame prepared above:

```{r}
data
```

The calculations in the previous sections were implemented using
for-loops in order to more closely match the equations given previously.
In the functions, we'll use a vectorized implementation instead.

```{r}
tidiness <- function(data) {
  data_with_probabilities <- weight_and_aggregate(data) %>%
    add_probabilities()
  I <- mutual_information(data_with_probabilities)
  H <- joint_entropy(data_with_probabilities)
  I / H
}

weight_and_aggregate <- function(data) {
 mutate(
    data,
    # compute weights
    WEIGHT=BELONGS_TO_CLUSTER * abs(LOADING_ON_FACTOR)
  ) %>%
    # drop rows with WEIGHT = 0
    filter(WEIGHT > 0) %>%
    # drop unnecessary information
    select(-FEATURE, -BELONGS_TO_CLUSTER, -LOADING_ON_FACTOR) %>%
    # aggregate weights by cluster and factor
    group_by(CLUSTER, FACTOR) %>%
    summarize(WEIGHT=sum(WEIGHT)) %>%
    ungroup()
}

add_probabilities <- function(weighted_and_aggregated_data) {
  # compute joint prob. dist.
  mutate(weighted_and_aggregated_data, JOINT=WEIGHT / sum(WEIGHT)) %>%
    # compute marginal prob. dist. (clusters)
    group_by(CLUSTER) %>%
    mutate(MARGINAL_CLUSTER=sum(JOINT)) %>%
    ungroup() %>%
    # compute marginal prob. dist. (factors)
    group_by(FACTOR) %>%
    mutate(MARGINAL_FACTOR=sum(JOINT)) %>%
    ungroup()
}

mutual_information <- function(data_with_probabilities) {
  with(
    data_with_probabilities,
    sum(JOINT * log2(JOINT / (MARGINAL_CLUSTER * MARGINAL_FACTOR)))
  )
}

joint_entropy <- function(data_with_probabilities) {
  with(
    data_with_probabilities,
    -1 * sum(JOINT * log2(JOINT))
  )
}
```

Let's double check that we get the same results as previously. For
reference, here's the value we calculated manually above:

```{r}
TIDINESS
```

And here's the result of our function:

```{r}
tidiness(data)
```

Looks like we're good to go!

Testing if it works
===================

In order to see if *tidiness* does the job we expect it to do, we need
to compare its output on different data. The data we've worked with so
far is fairly tidy, `c1` is strongly associated with `f2`, `c2` with
`f1`, and the remaining associations are much weaker:

```{r}
aggregated
```

Let's make it messier. The only difference will be that now, feature `B`
will load comparably on both factors `f1` and `f2`:

```{r}
messier_data <- mutate(
  data,
  LOADING_ON_FACTOR=ifelse(FEATURE == "B" & FACTOR == "f1", .60, LOADING_ON_FACTOR)
)
messier_data
```

This is what `messier_data` looks like when weighted and aggregated:

```{r}
weight_and_aggregate(messier_data)
```

As you can see, the correspondence between `c1` and `f1` has become a
more serious competitor (its weight rose from 0.15 to 0.70) to the
correspondence between `c1` and `f2` (weight = 1.05). Let's see how
*tidiness* responds to that:

```{r}
# same as above, for easier comparison
tidiness(data)
```

```{r}
tidiness(messier_data)
```

```{r}
tidiness(data) > tidiness(messier_data)
```

Good, it seems to be working! The data we made messier on purpose is
indeed identified as less tidy than the original data. This makes sense,
because in `messier_data`, the structure of factors has become less
predictable based on the structure of clusters.

References
==========

Biber, Douglas. 1988. *Variation across speech and writing*.
  Cambridge: Cambridge University Press.

Cvrček, Václav, Zuzana Komrsková, David Lukeš, Petra Poukarová, Anna
  Řehořková, Adrian Jan Zasina. 2018. From extra- to intratextual
  characteristics: Charting the space of variation in Czech through MDA.
  *Corpus Linguistics and Linguistic Theory*. Available on-line ahead of
  print via
  [doi:10.1515/cllt-2018-0020](https://doi.org/10.1515/cllt-2018-0020).

Revelle, William. 2017. *psych: Procedures for Personality and
  Psychological Research*. Northwestern University, Evanston, Illinois,
  USA. <https://CRAN.R-project.org/package=psych> Version = 1.7.8.
